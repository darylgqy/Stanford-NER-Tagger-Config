{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford NER Tagging\n",
    "\n",
    "NER stands for **Named Entity Recognition**. It is to extract information from unstructured text, basically by extraction a real world entity from the text (e.g. Person, Organization, Event, etc). This is to determine the relationships between different named entities.\n",
    "\n",
    "This guide is only relevant to Windows Operating System.\n",
    "Credits: Code methods belongs to Chuck Dishmon\n",
    "\n",
    "## Set-up NLTK in Python\n",
    "\n",
    "Firstly, please install NLTK via Anaconda:\n",
    "- [Anaconda]https://repo.anaconda.com/archive/Anaconda3-5.2.0-Windows-x86_64.exe\n",
    "\n",
    "The above link will install a distribution of python packages and applications which includes NLTK.\n",
    "\n",
    "After installing Anaconda, search and open Jupyter Notebook.\n",
    "Run this code:\n",
    "```python\n",
    "import nltk\n",
    "nltk.download(\"punkt\")```\n",
    "\n",
    "## Set-up Java for Stanford NER\n",
    "Given Stanford NER runs on Java, we will also need Java Runtime Environment (JRE) in order to use NLTK a a python parser. <br>\n",
    "To download (only if you do not have Java installed):\n",
    "- [Java] https://java.com/en/download/\n",
    "\n",
    "\n",
    "## Set-up Stanford NER for Python\n",
    "\n",
    "Stanford NER is a Java implementation of a Named Entity Recognizer. NER labels sequences of words in a text whih are the names of things, such as person and company names, or gene and protein names. It has good named entity recognizers for English, particularly for the 3 classes (PERSON, ORGANIZATION, LOCATION) and even for other languages.\n",
    "\n",
    "Once you have installed NLTK python, please download and extract the following Stanford files into separate folders:\n",
    "\n",
    "1. [Stanford Parser] https://nlp.stanford.edu/software/stanford-parser-full-2018-02-27.zip\n",
    "2. [Stanford NER] https://nlp.stanford.edu/software/stanford-ner-2018-02-27.zip\n",
    "3. [Stanford Log-Linear POS Tagger] https://nlp.stanford.edu/software/stanford-postagger-full-2018-02-27.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "### 1. Environment Variables Window\n",
    "Once you have all relevant packages and files, go to the search panel and search for \"Environment Variables\"\n",
    "\n",
    "### 2. Create two new variables\n",
    "\n",
    "#### CLASSPATH\n",
    "Create a CLASSPATH user variable by clicking on the button New then add the following values\n",
    "```\n",
    "Drive:\\path\\to\\stanford-ner-2018-02-27\n",
    "Drive:\\path\\to\\stanford-parser-full-2018-02-27\n",
    "Drive:\\path\\to\\stanford-postagger-2018-02-27\n",
    "```\n",
    "Example:\n",
    "```\n",
    "C:\\Users\\Daryl\\Desktop\\Stanford NER\\stanford-ner-2018-02-27\n",
    "C:\\Users\\Daryl\\Desktop\\Stanford NER\\stanford-parser-full-2018-02-27\n",
    "C:\\Users\\Daryl\\Desktop\\Stanford NER\\stanford-postagger-full-2018-02-27\n",
    "```\n",
    "\n",
    "#### STANFORD_MODELS\n",
    "Create another user variable named STANFORD_MODELS and add the following values\n",
    "```\n",
    "Drive:\\path\\to\\stanford-postagger-2018-02-27\\models\n",
    "Drive:\\path\\to\\stanford-ner-2018-02-27\\classifiers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Stanford NER\n",
    "\n",
    "```python\n",
    "#import packages\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "```\n",
    "Please find the correct type of files for the paths and add an r (raw string literal) before the quotation marks in the paths.\n",
    "```python\n",
    "#Configurations for py-java and setting paths for stanfordNERTagger\n",
    "import os\n",
    "java_path = r\"C:/Program Files/Java/jre1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "stanford_ner_path = r\"C:/Users/Daryl/Desktop/Stanford NER/stanford-ner-2018-02-27/stanford-ner-2018-02-27/stanford-ner-3.9.1.jar\"\n",
    "stanford_classifier = r\"C:/Users/Daryl/Desktop/Stanford NER/stanford-ner-2018-02-27/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz\"\n",
    "```\n",
    "\n",
    "### Methods\n",
    "\n",
    "```python\n",
    "#Process and tokenize the words in the text file\n",
    "def process_text(text_file):\n",
    "    text = open(text_file,errors = \"ignore\")\n",
    "    text = text.read()\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    return tokenized_text\n",
    "```\n",
    "\n",
    "```python\n",
    "#Create a Stanford NER Tagger object and use to tag each tokenized word\n",
    "def stanford_tagger(tokenized_text):\n",
    "    #create tagger \n",
    "    st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "    #tag every tokenized word\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    return classified_text\n",
    "```\n",
    "\n",
    "```python\n",
    "#Tag tokens with standard NLP BIO tags\n",
    "def bio_tagger(classified_text):\n",
    "    bio_tagged = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in classified_text:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged.append((token,tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": #Begin NE\n",
    "            bio_tagged.append((token, \"B-\" + tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: #Inside NE\n",
    "            bio_tagged.append((token, \"I-\" + tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: #Adjacent NE\n",
    "            bio_tagged.append((token, \"B-\" + tag))\n",
    "            prev_tag = tag\n",
    "    return bio_tagged\n",
    "```\n",
    "\n",
    "```python\n",
    "#Create Chunk Tree\n",
    "def stanford_tree(bio_tagged):\n",
    "    tokens, ne_tags = zip(*bio_tagged)\n",
    "    pos_tags = [pos for token, pos in pos_tags(tokens)]\n",
    "    conlltags = [(token, pos, ne) for token, pos, ne in zip(tokens, pos_tags, ne_tags)]\n",
    "    ne_tree = conlltags2tree(conlltags)\n",
    "    return ne_tree\n",
    "```\n",
    "\n",
    "```python\n",
    "#Parse named entities from tree\n",
    "def structure_ne(ne_tree):\n",
    "    ne = []\n",
    "    for subtree in ne_tree:\n",
    "        #if subtree is a noun chunk; NE != \"O\"\n",
    "        if type(subtree) == Tree: \n",
    "            ne_label = subtree.label()\n",
    "            ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            ne.append((ne_string, ne_label))\n",
    "    return ne\n",
    "```\n",
    "\n",
    "```python\n",
    "#Process the whole tagging by calling the methods\n",
    "def main():\n",
    "    tokenized_text = process_text(r\"C:\\Users\\Daryl\\Desktop\\Data Analysis\\Stanford NER\\text1.txt\")\n",
    "    tagged = stanford_tagger(tokenized_text)\n",
    "```\n",
    "\n",
    "```Sample Output:\n",
    "[('``', 'O'),\n",
    " ('Singapore', 'LOCATION'),\n",
    " ('has', 'O'),\n",
    " ('also', 'O'),\n",
    " ('registered', 'O'),\n",
    " ('our', 'O'),\n",
    " ('concerns', 'O'),\n",
    " ('with', 'O'),\n",
    " ('the', 'O'),\n",
    " ('relevant', 'O'),\n",
    " ('US', 'LOCATION'),\n",
    " ('and', 'O'),\n",
    " ('China', 'LOCATION'),\n",
    " ('departments', 'O'),\n",
    " ('and', 'O'),\n",
    " ('is', 'O'),\n",
    " ('continuing', 'O'),\n",
    " ('to', 'O'),\n",
    " ('engage', 'O'),\n",
    " ('them', 'O'),\n",
    " (',', 'O'),\n",
    " (\"''\", 'O'),\n",
    " ('said', 'O'),\n",
    " ('the', 'O'),\n",
    " ('spokesperson', 'O'),\n",
    " (',', 'O'),\n",
    " ('who', 'O'),\n",
    " ('did', 'O'),\n",
    " ('not', 'O'),\n",
    " ('respond', 'O'),\n",
    " ('to', 'O'),\n",
    " ('a', 'O'),\n",
    " ('query', 'O'),\n",
    " ('on', 'O'),\n",
    " ('the', 'O'),\n",
    " ('number', 'O'),\n",
    " ('of', 'O'),\n",
    " ('firms', 'O'),\n",
    " ('affected', 'O'),\n",
    " ('.', 'O'),\n",
    " ('In', 'O'),\n",
    " ('January', 'O'),\n",
    " (',', 'O'),\n",
    " ('US', 'LOCATION'),\n",
    " ('President', 'O'),\n",
    " ('Donald', 'PERSON'),\n",
    " ('Trump', 'PERSON')]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
